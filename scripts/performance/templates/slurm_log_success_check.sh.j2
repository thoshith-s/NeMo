#!/bin/bash
#
# Generated by NeMo Run (with log-based exit code validation)
# Run with: {{sbatch_command}}
#

# Parameters
{%- for sbatch_flag in sbatch_flags %}
{{sbatch_flag}}
{%- endfor %}

set -evx

export PYTHONUNBUFFERED=1
export SLURM_UNBUFFEREDIO=1
export TORCHX_MAX_RETRIES={{max_retries}}

set +e

# setup

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
{{head_node_ip_var}}=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)
{% if heterogeneous %}
{% for i in range(srun_commands|length) %}
{{het_group_host_var}}_{{i}}=$(scontrol show hostnames=$SLURM_JOB_NODELIST_HET_GROUP_{{i}} | head -n1)
{%- endfor %}
{% endif %}

{%- for env_var in env_vars %}
{{env_var}}
{%- endfor %}

{%- if setup_lines %}
{{setup_lines}}
{%- endif %}

{%- if ft_enabled %}
{{ fault_tolerance.ft_launcher_setup(fault_tol_cfg_path, fault_tol_finished_flag_file, fault_tol_job_results_file) }}
{%- endif %}

{%- if memory_measure %}
srun --ntasks=1 --ntasks-per-node=1 --output {{memory_measure}} --wait=60 --kill-on-bad-exit=1 --overlap nvidia-smi
{%- endif %}

{% for srun_command in srun_commands %}
# Command {{loop.index}}

{%- if loop.index <= group_env_vars|length %}
{% for env_var in group_env_vars[loop.index - 1] %}
{{env_var}}
{% endfor %}
{%- endif %}

{{srun_command}}
{% endfor %}

{%- if monitor_group_job %}

# The code below monitors all SLURM jobs to ensure any failure forces them all to stop
# (otherwise some jobs may remain pending until they reach the cluster time limit).
all_done=false
while ! $all_done; do
    all_done=true
    for pid in "${pids[@]}"; do
        if ps -p "$pid" > /dev/null; then
            # Process is still running.
            all_done=false
        else
            # Process is no longer running => check its exit status.
            wait "$pid"
            exitcode=$?
            echo "Process $pid exited with code $exit_code at $(date '+%Y-%m-%d %H:%M:%S')"
            # Wait a bit (to get a clean stack trace in case there is one being generated), then kill the
            # remaining processes if needed.
            sleep {{monitor_group_job_wait_time}}
            for other_pid in "${pids[@]}"; do
                if ps -p "$other_pid" > /dev/null; then
                    echo "Killing process $other_pid"
                    kill -9 "$other_pid"
                fi
            done
            break 2
        fi
    done

    # Sleep for a while before checking again.
    sleep {{monitor_group_job_wait_time}}
done
{% elif run_as_group %}
wait

exitcode=$?
{% else %}
exitcode=$?
{% endif %}
set -e

echo "job exited with code $exitcode"

{%- if ft_enabled %}
{{ fault_tolerance.ft_launcher_teardown() }}
{%- else %}
# ============================================================================
# LOG-BASED SUCCESS VALIDATION (Temporary Workaround)
# ============================================================================
# Background: Some jobs report non-zero exit codes despite successful completion.
# This section checks training logs for success patterns and overrides the exit
# code if training actually completed successfully.
# ============================================================================

if [ $exitcode -ne 0 ]; then
    echo "[LOG_CHECK] Job failed with exit code $exitcode - validating logs..."
    
    # Construct log file pattern: log-{job_name}_{SLURM_JOB_ID}_{SLURM_RESTART_COUNT}.out
    LOG_DIR="{{log_dir}}"
    JOB_NAME="{{job_name}}"
    LOG_PATTERN="${LOG_DIR}/log-${JOB_NAME}_${SLURM_JOB_ID}_*.out"
    
    # Find the most recent log file (highest restart count)
    LATEST_LOG=""
    MAX_RESTART_COUNT=-1
    
    for log_file in $LOG_PATTERN; do
        if [ -f "$log_file" ]; then
            # Extract restart count from filename: log-prefix_jobid_RESTART.out
            RESTART_COUNT=$(basename "$log_file" | sed 's/.*_\([0-9]*\)\.out$/\1/')
            if [ "$RESTART_COUNT" -gt "$MAX_RESTART_COUNT" ]; then
                MAX_RESTART_COUNT=$RESTART_COUNT
                LATEST_LOG="$log_file"
            fi
        fi
    done
    
    if [ -n "$LATEST_LOG" ] && [ -r "$LATEST_LOG" ]; then
        # Success pattern: "Trainer.fit` stopped: `max_steps=<number>` reached."
        if grep -E -q 'Trainer\.fit.*stopped:.*max_steps=.*reached\.' "$LATEST_LOG"; then
            echo "[LOG_CHECK] ✓ Training completed successfully (max_steps reached)"
            echo "[LOG_CHECK] Overriding exit code: $exitcode -> 0"
            exitcode=0
        else
            echo "[LOG_CHECK] ✗ Success pattern not found - genuine failure"
        fi
    else
        if [ -z "$LATEST_LOG" ]; then
            echo "[LOG_CHECK] ⚠ No log files found: $LOG_PATTERN"
        else
            echo "[LOG_CHECK] ⚠ Cannot read log file: $LATEST_LOG"
        fi
        echo "[LOG_CHECK] Unable to validate - assuming genuine failure"
    fi
fi

# ============================================================================
# END LOG-BASED SUCCESS VALIDATION
# ============================================================================


if [ $exitcode -ne 0 ]; then
    if [ "$TORCHX_MAX_RETRIES" -gt "${SLURM_RESTART_COUNT:-0}" ]; then
        scontrol requeue "$SLURM_JOB_ID"
    fi
    exit $exitcode
fi
{%- endif %}
